# Metric

要理解从句子到相似度矩阵的转换，我们可以将其分为几个关键步骤：文本编码、模型处理、嵌入池化、向量规范化，以及计算余弦相似度。以下是每个步骤的详细解释：

1. 文本编码（Tokenization）
首先，文本（句子）需要被转换成模型可以理解的格式。这通常包括以下步骤：

分词（Tokenization）：句子被分解为词汇单元，这些词汇单元可能是单词、子词或符号。
转换为索引（Indexing）：每个词汇单元被转换为词汇表中的一个唯一索引。
添加特殊标记：如添加开始（[CLS]）和结束（[SEP]）标记，这对某些模型（如BERT）来说是必需的。
注意力掩码（Attention Mask）：这是一个向量，用于指示哪些索引是实际词汇单元，哪些是填充（Padding）。
2. 模型处理
使用如BERT或MiniLM这类预训练模型，输入的索引会转换为连续的嵌入表示。这个过程涉及：

词嵌入：将词汇索引转换为预定义的嵌入向量。
位置编码：每个词嵌入都添加位置信息，帮助模型理解词语在句子中的顺序。
层叠的Transformer层：这些层通过自注意力机制和前馈网络处理词嵌入，捕捉词汇之间的复杂关系。
3. 嵌入池化（Pooling）
因为Transformer输出的是每个词汇的嵌入，而我们需要的是整个句子的嵌入，所以要通过池化操作将单词级别的嵌入聚合成句子级别的嵌入。常见的池化方法是：

平均池化（Mean Pooling）：对所有词嵌入进行加权平均，其中权重由注意力掩码确定（忽略掉填充词汇的影响）。
4. 向量规范化
在得到句子的嵌入后，通过规范化（通常是L2规范化）处理这些嵌入。规范化确保嵌入向量长度为1，这有助于后续计算相似度时减少向量长度带来的影响。

5. 计算余弦相似度
最后，计算句子嵌入之间的余弦相似度：

余弦相似度：衡量两个向量在方向上的接近程度，计算公式为两向量点乘的结果除以两向量的模的乘积。
最终的结果是一个相似度矩阵，其中矩阵的每个元素（i,j）表示第i个句子与第j个句子的相似度。这个矩阵是对称的，对角线元素都是1，因为每个句子与其自身的相似度最大。

这个流程把原始的文本数据转换为可以用于各种应用（如文档聚类、信息检索等）的数值形式，使得可以用算法处理和分析。

```bash
python text2image.py --task classify
```


```bash
python text2image.py --task visualize
```